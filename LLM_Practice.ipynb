{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07122ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b07676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d068c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Tequila Palace\n"
     ]
    }
   ],
   "source": [
    "# Using LLM model from OpenAI\n",
    "llm = OpenAI(temperature=0.6)\n",
    "name = llm(\"Suggest a name for a mexican resturant\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4c592f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suggest a name for a Italian resturant'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Prompt Templates\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \"Suggest a name for a {cuisine} resturant\"\n",
    ")\n",
    "\n",
    "prompt_template_name.format(cuisine = \"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae625f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpice of India'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using LLM Chains\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \"Suggest a name for a {cuisine} resturant\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt= prompt_template_name)\n",
    "name_chain.run(\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcb3093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSoft Tacos, Hard Tacos, Burritos, Nachos, Fajitas, Quesadillas, Enchiladas, Chalupas, Tamales, Tostadas, Tortas'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Simple Sequential Chain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \"Suggest a name for a {cuisine} resturant\"\n",
    ")\n",
    "name_chain = LLMChain(llm=llm, prompt= prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}. Return it as comman separated.\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm = llm, prompt=prompt_template_items)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "sequential_chain.run(\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7294499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Arabic',\n",
       " 'restaurant_name': '\\n\\nAl-Muhajirah Kitchen',\n",
       " 'menu_items': '\\n\\nGrilled Chicken, Beef Kebab, Lamb Curry, Vegetable Samosa, Biryani Rice, Mint Chutney, Naan Bread, Raita, Pakoras, Falafel, Vegetable Biryani, Lentil Soup, Mango Lassi, Gulab Jamun.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Sequential Chain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "llm = OpenAI(temperature = 0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \"Suggest a name for a {cuisine} resturant\"\n",
    ")\n",
    "name_chain = LLMChain(llm=llm, prompt= prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}. Return it as comman separated.\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm = llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "chain({'cuisine': 'Arabic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Agents in LLM\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"When was Elon Musk born? What is his age right now in 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a29f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n",
      "\n",
      "\n",
      "Gusto Italiano Ristorante\n",
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "Taco Fiesta\n",
      "Human: Italian\n",
      "AI: \n",
      "\n",
      "Gusto Italiano Ristorante\n"
     ]
    }
   ],
   "source": [
    "# Using Memory\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= [\"cuisine\"],\n",
    "    template= \"Suggest a name for a {cuisine} resturant\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt= prompt_template_name, memory=memory)\n",
    "\n",
    "name = chain.run(\"Mexican\")\n",
    "# print(name)\n",
    "\n",
    "name = chain.run(\"Italian\")\n",
    "# print(name)\n",
    "\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b12d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using conversation chain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "\n",
    "convo.run(\"Who won the first cricket world cup?\")\n",
    "convo.run(\"What is 5 + 5\")\n",
    "convo.run(\"Who was the captain of the winning team?\")\n",
    "\n",
    "convo.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2598ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Conversation Buffer Window Memory\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "\n",
    "convo.run(\"Who won the first cricket world cup?\")\n",
    "convo.run(\"What is 5 + 5\")\n",
    "convo.run(\"Who was the captain of the winning team?\")\n",
    "\n",
    "convo.memory.buffer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
